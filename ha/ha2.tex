\subsection*{Homework 2}

\begin{enumerate}
	\item \textbf{[10pt]} Рассмотренные методы исправления опечаток не работают напрямую при 
	пропуске пробела (например, \textit{informationretrieval}). Опишите, как исправлять такие 
	опечатки (не обязательно на основе рассмотренных методов).
	
	\textit{Решение.} Можно пойти двумя путями: увеличить размер индекса и время обработки запроса
	\begin{itemize}
		\item увеличение размера индекса - линейное увеличение - рассматривать как слова ещё и 
		пары слов, которые были соседними в исходном документе. Полиномиальное увеличение размера 
		индекса - рассматривать все пары/тройки/четверки/... слов, которые могут образовывать 
		слова. Наверное на практике такое сложно применить, хотя при небольшом словаре этот 
		способ может быть рабочим.  
		\item увеличение времени на обработку запроса. Пытаем разбить слово на несколько слов 
		всеми возможными способами (сначала на 2, потом на 3 и так до какого-то разумного 
		лимита). Так же можно пытаться найти соответствие префикса какому-либо слову из словаря, 
		и потом рекурсивно повторить операцию на суффиксе.
	\end{itemize}
	
	\item \textbf{[5pt]} Мы рассмотрели два типа методов для рекомендации запросов, аналогичных 
	заданному запросу:
	\begin{enumerate}
		\item Рекомендовать запросы, встречающиеся в одной сессии с заданным запросом.
		\item Рекомендовать те запросы, у которых множество кликнутых результатов сильно 	
		пересекается с аналогичным множеством для заданного запроса.
	\end{enumerate}

	Какие еще методы для рекомендации запросов вы можете предложить?
	
	\textit{Решение.} 
	\begin{itemize}
		\item Запросы других людей, за последние $N$ минут, которые похожи на текущий запрос 
		(например, пересекаются по некоторым словам). Пример: $q=$\textit{Презентация ...}, 
		возможная подсказка \textit{"Презентация apple"}. Т.к. она (условно) была вчера вечером и 
		уже многие сегодня весь день хотели найти об этом информацию.
		\item Запросы других людей, которые были в это же время суток/в тот день недели/в таких 
		же числах месяца/в то же время года, которые похожи на текущий запрос (например, 
		пересекаются по некоторым словам). Пример: пятница, полдень $q=$ \textit{"погода на "}. 
		Логичнее дополнить как \textit{"на выходные"}. То же самое работает и для праздников.
		\item Запросы которые уже встречались от этого же пользователя и похож на текущий. Часто 
		бывает нужно повторить поиск, но не всегда пользователь запоминает абсолютно точно текст 
		запроса, но это может влиять на выдачу.
		\item Если известна геопозиция пользователя(либо история запросов с какими-либо 
		гео-данными), то можно дополнить запрос соответствующей информацией.
		
		Пример: $q=$\textit{Погода в}. Возможная рекомендация \textit{Погода в Турции}, если он уже 
		упоминал Турцию, когда искал билеты и отель.
	\end{itemize}
	\item \textbf{[5pt]} Вы планируете использовать следующие методы поиска: модель векторного 
	пространства с весами TF-IDF, BM25, языковую модель. Какую минимальную информацию должен 
	содержать индекс, чтобы поддерживать эффективное использование этих методов? Какую информацию 
	нет смысла хранить в индексе? Как ее нужно хранить? Дайте развернутый ответ.
	
	\textit{Решение.} TODO: Add parameters for the language model 
	
	Рассмотрим используемые параметры в этих методах.
	\begin{itemize}
		\item $tf(t, d)$ - частота слова внутри документа (term frequency)
		\item $df(t)$ - частота слова среди всех документов
		\item $ifd(t)$ - обратная частота слова
		\item $dl(d)$ - длина документа
		\item $dl_{ave}$ - средняя длина документа
		\item $N$ - количество всех документов
	\end{itemize}
	
	Несложно догадаться, что индекс должен содержать хотя бы отображение из слова в список 
	документов, в которых оно встречается, а так же количество вхождений этого слова в 
	соответствующий документ. Назовём этот индекс $I$. Это позволит вычислить $tf(t, d)$ - найти 
	$I(t)$ документ $d$ и взять количество вхождений. $df(t)$ - это просто длина списка $I(t)$. 
	
	Для вычисления оставшихся характеристик, можно включить в индекс отображение $D$ - документ 
	$\rightarrow$ его длина. Ещё нужно хранить суммарную длину всех документов - $L$. Тогда:
	\begin{align*}
		dl(d) &= D(d) \\
		N &= |D|\\
		dl_{ave} &=\frac{L}{N}
	\end{align*}
	Теперь, зная $N$, можно вычислить $idf(t)$ по понравившейся формуле.
	
	Любая дополнительная информация будет избыточной. Например, лишено смысла хранить $dl_{ave}$ в 
	явном виде, т.к. при добавлений документа его будет сложно обновить.
	
	Первый индекс ($I$) можно хранить как хеш-таблицу, второй индекс ($D$) как обычный 
	массив(файл) с прямой адресацией, где индекс - это идентификатор документа (тогда id 
	документов нужно выбрать соответствующим образом - как целые неотрицательные числа)
	
	\item \textbf{[10pt]} Отранжируйте документы из таблицы 1 по запросу "car insurance" с 
	использованием модели векторного пространства и весов TF-IDF.
	
	\begin{tabular}{c | c | c c c}
		\multirow{2}{*}{Слово} & \multirow{2}{*}{idf} & \multicolumn{3}{c}{tf} \\ \cline{3-5}
		& & Документ 1 & Документ 2 & Документ 3 \\ \hline	
		car & 1.65 & 27 & 4 & 24 \\
		auto & 2.08 & 3 & 33 & 0 \\
		insurance & 1.62 & 0 & 33 & 29 \\
		best & 1.60 & 14 & 0 & 17 \\
	\end{tabular}
	
	\textit{Решение.} Определим веса TF-IDF для каждого из документов ($d_i = v (document \ i)$).
	\begin{align*}
		d_1 &= \{27 \cdot 1.65, 3 \cdot 2.08, 0 \cdot 1.62, 14 \cdot 1.60\} = \{44.55, 6.24, 
		0.00, 22.40\} \\
		d_2 &= \{4 \cdot 1.65, 33 \cdot 2.08, 33 \cdot 1.62, 0 \cdot 1.60\} = \{6.60, 68.64, 
		53.46, 0.00\} \\
		d_3 &= \{24 \cdot 1.65, 0 \cdot 2.08, 29 \cdot 1.62, 17 \cdot 1.60\} = \{39.60, 0.00, 
		46.98, 27.20\}
	\end{align*}
	
	А так же вектор для запроса: 
	\begin{align*}
		v(q) = \{1.65, 0.00, 1.62, 0.00\}
	\end{align*}
	
	Осталось вычислить значения $sim(d_i, v(q))$ и ранжировать запросы.
	\begin{equation*}
		sim (d_i, v(q)) = \frac{d_i \cdot v(q)}{||d_i||\cdot ||v(q)||}
	\end{equation*}
	\begin{align*}
		sim(d_1, v(q)) &= \frac{44.55 \cdot 1.65}{\sqrt{44.55^2 + 6.24^2 + 22.40^2} \cdot 
				\sqrt{1.65^2 + 1.62^2}} = 0.63 \\
		sim(d_2, v(q)) &= \frac{6.60 \cdot 1.65 + 53.46\cdot 1.62}{\sqrt{6.60^2 + 68.64^2 + 
				53.46^2} \cdot \sqrt{1.65^2 + 1.62^2}} = 0.48 \\
		sim(d_3, v(q)) &= \frac{39.60 \cdot 1.65 + 46.98\cdot 1.62}{\sqrt{39.60^2 + 46.98^2 + 
				27.20^2} \cdot \sqrt{1.65^2 + 1.62^2}} = 0.91 \\
	\end{align*}
	Таким образом, ранжирование будет следующим:
	\begin{enumerate}
		\item[1:] Документ 3
		\item[2:] Документ 1
		\item[3:] Документ 2
	\end{enumerate}
	
	\item \textbf{[5pt]} Рассмотрим следующий запрос и три результата.
	\begin{enumerate}
		\item[Q] information retrieval course
		\item[D1] Information Retrieval and Web Search
		\item[D2] Introduction to Information Retrieval
		\item[D3] Text Retrieval and Search Engines
	\end{enumerate}
	
	Результаты $1$ и $3$ – это страницы соответствующих курсов, поэтому пользователь пометил их 
	как релевантные. Результат $2$ – это страница с книгой, поэтому пользователь пометил его как 
	нерелевантный.
	
	Примените алгоритм Роккио и выпишите вектор запроса после учета обратной связи по 
	релевантности. Элементы вектора перечислите в алфавитном порядке. Считайте, что компоненты 
	векторов содержат только частоты слов (без обратной документной частоты и нормировки). 
	Параметры алгоритма Роккио: $\alpha = 1, \beta = 0.75, \gamma = 0.25.$

	\textit{Решение. } Все что нам нужно-воспользоваться формулой:
	\begin{align*}
		q_{opt} = \alpha v(q) + \beta \mu(D_r) - \gamma \mu(D_{nr})
	\end{align*}
	
	Выпишем таблицу с частотами слов в документах и запросе
	
	\begin{tabular}{l | c c c | c}
						& \textbf{D1} & D2 & \textbf{D3} & Q \\ \hline
		and 			& 1 & 0 & 1 & 0 \\
		course 			& 0 & 0 & 0 & 1 \\
		engines		 	& 0 & 0 & 1 & 0\\
		information		& 1 & 1 & 0 & 1 \\
		introduction 	& 0 & 1 & 0 & 0\\\hline
		retrieval 		& 1 & 1 & 1 & 1 \\
		search 			& 1 & 0 & 1 & 0 \\
		text 			& 0 & 0 & 1 & 0\\ 
		to			 	& 0 & 1 & 0 & 0\\
		web 			& 1 & 0 & 0 & 0 \\
	\end{tabular}
	
	Таким образом, получили следующие вектора:
	\begin{align*}
		v(D1) &= \{1, 0, 0, 1, 0, 1, 1, 0, 0, 1\} \\
		v(D2) &= \{0, 0, 0, 1, 1, 1, 0, 0, 1, 0\} \\
		v(D3) &= \{1, 0, 1, 0, 0, 1, 1, 1, 0, 0\} \\
		v(q) &= \{0, 1, 0, 1, 0, 1, 0, 0, 0, 0\} \\
	\end{align*}
	
	Для вычисления $q_{opt}$ потребуются $D_{r}, D_{nr}$ - множество векторов для релевантных и 
	нерелевантных запросов.
	\begin{align*}
		D_r = \{v(D1), v(D3)\}
		D_{nr} = \{v(D2)\}
	\end{align*}
	
	Вычислим $\mu(D_r)$:
	\begin{align*}
		\mu(D_r) = \frac{1}{2} (v(D1) + v(D3)) = \frac{1}{2}\cdot (\{1, 0, 0, 1, 0, 1, 1, 0, 
		0, 1\} + \{1, 0, 1, 0, 0, 1, 1, 1, 0, 0\}) = \\
		= \{1, 0, 0.5, 0.5, 0, 1, 1, 0.5, 0, 0.5\}
	\end{align*}
	
	И $\mu(D_{nr})$:
	\begin{equation*}
	\mu(D_{nr}) = v(D2) = \{0, 0, 0, 1, 1, 1, 0, 0, 1, 0\}
	\end{equation*}
	
	Осталось вычислить результат
	
	\begin{align*}
		q_{opt} = \alpha v(q) + \beta \mu(D_r) - \gamma \mu(D_{nr}) = 1 \cdot q + 0.75 \cdot 
		\mu(D_r) - 0.25 \cdot \mu(D_{nr}) = 
		= \{0, 1, 0, 1, 0, 1, 0, 0, 0, 0\} + \\ + 0.75 \cdot \{1, 0, 0.5, 0.5, 0, 1, 1, 0.5, 
		0, 0.5\} - 0.25 \cdot \{0, 0, 0, 1, 1, 1, 0, 0, 1, 0\} = \\ = (0.75, 1, 0.375, 
		1.125, -0.25, 1.5, 0.75, 0.375, -0.25, 0.375)
	\end{align*}

	\item \textbf{[10pt]} Выпишите формулу \textit{BM25} для длинных запросов. Опишите ее 
	составляющие. Каким образом каждая составляющая влияет на ранжирование (т.е. что происходит	с 
	ранжированием результатов при изменении каждой из составляющих)?

	\textit{Решение. } Формула для $BM25$ выглядит так:
	\begin{align*}
	\sum\limits_{t \in q} \log \left[ \frac{N}{df(t)} \right] \cdot \frac{(k_1 + 1) \cdot 
		tf(t, d)}{k_1\cdot \left[ (1 - b) + b \frac{dl(d)}{dl_{ave}} \right] + tf(t, d)} 
	\cdot \frac{(k_3 + 1) \cdot tf(t, q)}{k_3 + tf(t, q)}
	\end{align*}
	
	Составляющие:
	\begin{itemize}
		\item $q$ - запрос
		\item $N$ - количество документов в коллекции
		\item $tf(t, d)$ - количество вхождений терма $t$ в документ $d$
		\item $df(t)$ - количество документов в коллекции, в которые входит терм $t$
		\item $dl(d)$ - длина документа $d$
		\item $dl_{ave}$ - средняя длина документа во всей коллекции
		\item $tf(t, q)$ - количество вхождений терма $t$ в запрос $q$
		\item $k_1, b, k_3$ - параметры метода. 
	\end{itemize}
	
	Влияние
	\begin{itemize}
		\item $N, df(t)$ - по-отдельности про них мало что можно сказать, лучше рассмотреть их 
		отношение
		\item  $\dfrac{N}{df(t)}$ - не зависит от документа, устанавливает "вес" для терма $t$ - 
		чем больше, тем лучше терм (т.е лучше определяет требуемую выдачу). Иными словами, чем 
		больше значение - тем меньше документов, которые содержат этот терм. 
		\item $tf(t, d)$ - количество вхождений терма $t$ в документ $d$. Чем чаще встречается 
		терм в документе, тем этого документ лучше подходит.
		\item $dl(d), dl_{ave}$ - абсолютные величины так же мало о чем говорят, поэтому 
		рассмотрим относительную величину - отношение длины документа к средней длине
		\item $\dfrac{dl(d)}{dl_{ave}}$ - чем больше, тем ниже ранг документа. Если документ 
		очень длинный, то в нем может быть много слов из запроса, но не потому что он подходит, а 
		просто потому что он слишком длинный и содержит много слов. 
		\item $tf(t, q)$ - чем чаще слово встречается в тексте запроса тем сильнее пользователь 
		его ищет. Это тоже "вес" для терма.
		\item $k_1$ - повышаем вклад для значения $tf(t, d)$, то есть выше будут документы, 
		которые чаще содержат терм $t$
		\item $b$ - выглядит как некоторый "параметр релаксации" для отношения 
		$\frac{dl(d)}{dl_{ave}}$. Ближе к $1$ - хотим учитывать среднюю длину документа, ближе к 
		$0$ - не хотим. Средние значения учесть, но с каким-то дополнительным весом.
		\item $k_3$ повышаем вклад для значения $tf(t, q)$, термы от, которые часто встречаются в 
		теле запроса будут давать больший вклад в скор для документов
	\end{itemize}

	\item \textbf{[10pt]} Пусть бинарная случайная величина $X_t$ – это индикатор того, что слово 
	$t$	встречается в документе (т.е. $X_t = 1$, если слово $t$ есть в документе, и $X_t = 0$, 
	если слова $t$ нет в документе). $P_t = P(X_t = 1 \big| d)$ – это вероятность того, что слово 
	$t$ встречается в документе $d$. 
	
	Примените метод максимального правдоподобия \textit{(MLE)} для формального вычисления $P_t$ и 
	покажите, что $P_t = \dfrac{tf(t,d)} {dl(d)}$, где 
	\begin{itemize}
		\item $tf(t, d)$ – это частота слова $t$ в документе $d$, 
		\item $dl(d)$ – это длина документа $d$.
	\end{itemize}
	
	\textit{Решение. } В нашем случае случайная величина - дискретная с двумя значениями - $0, 
	1$. Значит, функция плотности для неё - $p_0 = P(X_t = 0 | d) = (1 - \theta)$, $p_1 = 
	P(X_t = 1 | d) = \theta$. Пусть у нас есть $n$ элементов из распределения нашей случайной 
	величины, выпишем функцию правдоподобия в общем виде:
	\begin{equation*}
		\mathcal{L}(x_i; \theta) = \theta^k(1-\theta^{n-k})
	\end{equation*}
	
	где $k$ - количество случайных $x_i$, которые совпали с $t$. Найдем максимум функции правдоподобия. 
	\begin{equation*}
		\mathcal{L}'_\theta = k\theta^{k - 1}(1 - \theta^{n - k}) - (n - k)\theta^k (1 - \theta^{n - k - 1})
	\end{equation*}
	
	Найдём точку экстремума при которой $\mathcal{L}'_\theta = 0$
	\begin{align*}
		&k\theta^{k - 1}(1 - \theta^{n - k}) - (n - k)\theta^k (1 - \theta^{n - k - 1}) &= 0 \\
		&\theta^{k - 1}(1 - \theta^{n - k - 1})\left[ k (1 - \theta) - (n - k)\theta \right] &= 0 \\
		 &k (1 - \theta) - (n - k)\theta &= 0 \\
		 &k - k \theta - n \theta + k\theta &= 0 \\
		 &k - n\theta &= 0 \\
		 \\
		 \Rightarrow \theta &= \dfrac{k}{n}
	\end{align*}
	
	Осталось убедиться, что это точка максимума - $\mathcal{L}(x_i; 0) =\mathcal{L}(x_i; 1) = 0$. 
	Далее, мы могли бы определять знаки производной слева/справа от найденной точки экстремума,но 
	можем ограничиться следующим замечанием: на отрезке $[0, 1]$ это единственный экстремум, 
	значение в нём положительное, поэтому найденная точка ($\theta = \frac{k}{n}$) максимизирует 
	функцию правдоподобия.
	
	Выбрав $n = dl(d)$, т.е. включив в выборку все слова документа, получим требуемое утверждение.
	
	\item \textbf{[5pt]} Рассмотрим коллекцию из двух документов.
	\begin{enumerate}
		\item[D1] A language model is a probability distribution over words or sequences of words.
		\item[D2] A language model is used in many natural language processing applications.
	\end{enumerate}

	Выпишите сглаженную униграмную языковую модель для каждого документа. Используйте сглаживание 
	\textit{Jelinek-Mercer} с параметром $\lambda = 0.5$. Отранжируйте эти документы по запросу 
	\textit{"many words"}.
\end{enumerate}
