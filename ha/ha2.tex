\subsection*{Homework 2}

\begin{enumerate}
	\item \textbf{[10pt]} Рассмотренные методы исправления опечаток не работают напрямую при 
	пропуске пробела (например, \textit{informationretrieval}). Опишите, как исправлять такие 
	опечатки (не обязательно на основе рассмотренных методов).
	
	\textit{Решение.} Можно пойти двумя путями: увеличить размер индекса и время обработки запроса
	\begin{itemize}
		\item увеличение размера индекса - линейное увеличение - рассматривать как слова ещё и 
		пары слов, которые были соседними в исходном документе. Полиномиальное увеличение размера 
		индекса - рассматривать все пары/тройки/четверки/... слов, которые могут образовывать 
		слова. Наверное на практике такое сложно применить, хотя при небольшом словаре этот 
		способ может быть рабочим.  
		\item увеличение времени на обработку запроса. Пытаем разбить слово на несколько слов 
		всеми возможными способами (сначала на 2, потом на 3 и так до какого-то разумного 
		лимита). Так же можно пытаться найти соответствие префикса какому-либо слову из словаря, 
		и потом рекурсивно повторить операцию на суффиксе.
	\end{itemize}
	
	\item \textbf{[5pt]} Мы рассмотрели два типа методов для рекомендации запросов, аналогичных 
	заданному запросу:
	\begin{enumerate}
		\item Рекомендовать запросы, встречающиеся в одной сессии с заданным запросом.
		\item Рекомендовать те запросы, у которых множество кликнутых результатов сильно 	
		пересекается с аналогичным множеством для заданного запроса.
	\end{enumerate}

	Какие еще методы для рекомендации запросов вы можете предложить?
	
	\textit{Решение.} 
	\begin{itemize}
		\item Запросы других людей, за последние $N$ минут, которые похожи на текущий запрос 
		(например, пересекаются по некоторым словам). Пример: $q=$\textit{Презентация ...}, 
		возможная подсказка \textit{"Презентация apple"}. Т.к. она (условно) была вчера вечером и 
		уже многие сегодня весь день хотели найти об этом информацию.
		\item Запросы других людей, которые были в это же время суток/в тот день недели/в таких 
		же числах месяца/в то же время года, которые похожи на текущий запрос (например, 
		пересекаются по некоторым словам). Пример: пятница, полдень $q=$ \textit{"погода на "}. 
		Логичнее дополнить как \textit{"на выходные"}. То же самое работает и для праздников.
		\item Запросы которые уже встречались от этого же пользователя и похож на текущий. Часто 
		бывает нужно повторить поиск, но не всегда пользователь запоминает абсолютно точно текст 
		запроса, но это может влиять на выдачу.
		\item Если известна геопозиция пользователя(либо история запросов с какими-либо 
		гео-данными), то можно дополнить запрос соответствующей информацией.
		
		Пример: $q=$\textit{Погода в}. Возможная рекомендация $"Погода в Турции"$, если он уже 
		упоминал Турцию, когда искал билеты и отель.
	\end{itemize}
	\item \textbf{[5pt]} Вы планируете использовать следующие методы поиска: модель векторного 
	пространства с весами TF-IDF, BM25, языковую модель. Какую минимальную информацию должен 
	содержать индекс, чтобы поддерживать эффективное использование этих методов? Какую информацию 
	нет смысла хранить в индексе? Как ее нужно хранить? Дайте развернутый ответ.
	
	\textit{Решение.} TODO: Add parameters for the language model 
	
	Рассмотрим используемые параметры в этих методах.
	\begin{itemize}
		\item $tf(t, d)$ - частота слова внутри документа (term frequency)
		\item $df(t)$ - частота слова среди всех документов
		\item $ifd(t)$ - обратная частота слова
		\item $dl(d)$ - длина документа
		\item $dl_{ave}$ - средняя длина документа
		\item $N$ - количество всех документов
	\end{itemize}
	
	Несложно догадаться, что индекс должен содержать хотя бы отображение из слова в список 
	документов, в которых оно встречается, а так же количество вхождений этого слова в 
	соответствующий документ. Назовём этот индекс $I$. Это позволит вычислить $tf(t, d)$ - найти 
	$I(t)$ документ $d$ и взять количество вхождений. $df(t)$ - это просто длина списка $I(t)$. 
	
	Для вычисления оставшихся характеристик, можно включить в индекс отображение $D$ - документ 
	$\rightarrow$ его длина. Ещё нужно хранить суммарную длину всех документов - $L$. Тогда:
	\begin{align*}
		dl(d) &= D(d) \\
		N &= |D|\\
		dl_{ave} &=\frac{L}{N}
	\end{align*}
	Теперь, зная $N$, можно вычислить $idf(t)$ по понравившейся формуле.
	
	Любая дополнительная информация будет избыточной. Например, лишено смысла хранить $dl_{ave}$ в 
	явном виде, т.к. при добавлений документа его будет сложно обновить.
	
	Первый индекс ($I$) можно хранить как хеш-таблицу, второй индекс ($D$) как обычный 
	массив(файл) с прямой адресацией, где индекс - это идентификатор документа (тогда id 
	документов нужно выбрать соответствующим образом - как целые неотрицательные числа)
	\item \textbf{[10pt]} Отранжируйте документы из таблицы 1 по запросу "car insurance" с 
	использованием модели векторного пространства и весов TF-IDF.
	
	\begin{tabular}{c | c | c c c}
		\multirow{2}{*}{Слово} & \multirow{2}{*}{idf} & \multicolumn{3}{c}{tf} \\ \cline{3-5}
		& & Документ 1 & Документ 2 & Документ 3 \\ \hline	
		car & 1.65 & 27 & 4 & 24 \\
		auto & 2.08 & 3 & 33 & 0 \\
		insurance & 1.62 & 0 & 33 & 29 \\
		best & 1.60 & 14 & 0 & 17 \\
	\end{tabular}
	
	\textit{Решение.} Определим веса TF-IDF для каждого из документов ($d_i = v (document \ i)$).
	\begin{align*}
		d_1 &= \{27 \cdot 1.65, 3 \cdot 2.08, 0 \cdot 1.62, 14 \cdot 1.60\} = \{44.55, 6.24, 
		0.00, 22.40\} \\
		d_2 &= \{4 \cdot 1.65, 33 \cdot 2.08, 33 \cdot 1.62, 0 \cdot 1.60\} = \{6.60, 68.64, 
		53.46, 0.00\} \\
		d_3 &= \{24 \cdot 1.65, 0 \cdot 2.08, 29 \cdot 1.62, 17 \cdot 1.60\} = \{39.60, 0.00, 
		46.98, 27.20\}
	\end{align*}
	
	А так же вектор для запроса: 
	\begin{align*}
		v(q) = \{1.65, 0.00, 1.62, 0.00\}
	\end{align*}
	
	Осталось вычислить значения $sim(d_i, v(q))$ и ранжировать запросы.
	\begin{equation*}
		sim (d_i, v(q)) = \frac{d_i \cdot v(q)}{||d_i||\cdot ||v(q)||}
	\end{equation*}
	\begin{align*}
		sim(d_1, v(q)) &= \frac{44.55 \cdot 1.65}{\sqrt{44.55^2 + 6.24^2 + 22.40^2} \cdot 
				\sqrt{1.65^2 + 1.62^2}} = 0.63 \\
		sim(d_2, v(q)) &= \frac{6.60 \cdot 1.65 + 53.46\cdot 1.62}{\sqrt{6.60^2 + 68.64^2 + 
				53.46^2} \cdot \sqrt{1.65^2 + 1.62^2}} = 0.48 \\
		sim(d_3, v(q)) &= \frac{39.60 \cdot 1.65 + 46.98\cdot 1.62}{\sqrt{39.60^2 + 46.98^2 + 
				27.20^2} \cdot \sqrt{1.65^2 + 1.62^2}} = 0.91 \\
	\end{align*}
	Таким образом, ранжирование будет следующим:
	\begin{enumerate}
		\item[1:] Документ 3
		\item[2:] Документ 1
		\item[3:] Документ 2
	\end{enumerate}
	
	\item \textbf{[5pt]} Рассмотрим следующий запрос и три результата.
	\begin{enumerate}
		\item[Q] information retrieval course
		\item[D1] Information Retrieval and Web Search
		\item[D2] Introduction to Information Retrieval
		\item[D3] Text Retrieval and Search Engines
	\end{enumerate}
	
	Результаты $1$ и $3$ – это страницы соответствующих курсов, поэтому пользователь пометил их 
	как релевантные. Результат $2$ – это страница с книгой, поэтому пользователь пометил его как 
	нерелевантный.
	
	Примените алгоритм Роккио и выпишите вектор запроса после учета обратной связи по 
	релевантности. Элементы вектора перечислите в алфавитном порядке. Считайте, что компоненты 
	векторов содержат только частоты слов (без обратной документной частоты и нормировки). 
	Параметры алгоритма Роккио: $\alpha = 1, \beta = 0.75, \gamma = 0.25.$s

	\item \textbf{[10pt]} Выпишите формулу \textit{BM25} для длинных запросов. Опишите ее 
	составляющие. Каким образом каждая составляющая влияет на ранжирование (т.е. что происходит	с 
	ранжированием результатов при изменении каждой из составляющих)?

	\item \textbf{[10pt]} Пусть бинарная случайная величина $X_t$ – это индикатор того, что слово 
	$t$	встречается в документе (т.е. $X_t = 1$, если слово $t$ есть в документе, и $X_t = 0$, 
	если слова $t$ нет в документе). $P_t = P(X_t = 1 \big| d)$ – это вероятность того, что слово 
	$t$ встречается в документе $d$. 
	
	Примените метод максимального правдоподобия \textit{(MLE)} для формального вычисления $P_t$ и 
	покажите, что $P_t = \dfrac{tf(t,d)} {dl(d)}$, где 
	\begin{itemize}
		\item $tf(t, d)$ – это частота слова $t$ в документе $d$, 
		\item $dl(d)$ – это длина документа $d$.
	\end{itemize}
	
	\item \textbf{[5pt]} Рассмотрим коллекцию из двух документов.
	\begin{enumerate}
		\item[D1] A language model is a probability distribution over words or sequences of words.
		\item[D2] A language model is used in many natural language processing applications.
	\end{enumerate}

	Выпишите сглаженную униграмную языковую модель для каждого документа. Используйте сглаживание 
	\textit{Jelinek-Mercer} с параметром $\lambda = 0.5$. Отранжируйте эти документы по запросу 
	\textit{"many words"}.
\end{enumerate}
